## **Desafios**

### **An√°lise de Sentimentos/NLP**

#### **Objetivo**

##### Produzir um modelo de Machine Learning para classificar o sentimentos dos usu√°rios em rela√ß√£o √†s compras de acordo com os reviews dos produtos.

* Entreg√°veis
1. Data Pipeline para automatizar as etapas de ETL.
> 
2. Modelo de classifica√ß√£o que dado um determinado conjunto de teste o modelo consiga retornar a classifica√ß√£o em positivo, negativo ou neutro.
> 
3. Relat√≥rio com os insights gerados a partir dos dados.

Um sistema de an√°lise de sentimentos com conte√∫do textual combina processamento de linguagem natural (PNL/NLP) com t√©cnicas de Machine Learning
para conferir pontua√ß√µes ponderadas de sentimentos √† senten√ßas.

Visto que a intelig√™ncia artificial visa simular a estrutura de pensamento dos seres humanos, bem como permitir di√°logos complexos entre m√°quina e humano, 
o NLP √© indispens√°vel para permitir que a m√°quina compreenda o que est√° sendo dito e possa estruturar a melhor resposta.

Em suma, a intelig√™ncia artificial usa o processamento de linguagem natural para entender a linguagem humana e simul√°-la.

Al√©m do NLP, alguns outros conceitos est√£o inclu√≠dos no processamento da Intelig√™ncia Artificial, entre eles machine learning e deep learning NLP.


#### **Chamando as primeiras bibliotecas**


```python
import numpy as np
import pandas as pd
!pip install tabulate
```

    Collecting tabulate
      Using cached tabulate-0.8.9-py3-none-any.whl (25 kB)
    Installing collected packages: tabulate
    Successfully installed tabulate-0.8.9
    


```python
df_original = pd.read_csv("review.csv") # csv com os dados de review dos usu√°rios
df_original.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>review_id</th>
      <th>order_id</th>
      <th>review_score</th>
      <th>review_comment_title</th>
      <th>review_comment_message</th>
      <th>review_creation_date</th>
      <th>review_answer_timestamp</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>7bc2406110b926393aa56f80a40eba40</td>
      <td>73fc7af87114b39712e6da79b0a377eb</td>
      <td>4</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2018-01-18 00:00:00</td>
      <td>2018-01-18 21:46:59</td>
    </tr>
    <tr>
      <th>1</th>
      <td>80e641a11e56f04c1ad469d5645fdfde</td>
      <td>a548910a1c6147796b98fdf73dbeba33</td>
      <td>5</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2018-03-10 00:00:00</td>
      <td>2018-03-11 03:05:13</td>
    </tr>
    <tr>
      <th>2</th>
      <td>228ce5500dc1d8e020d8d1322874b6f0</td>
      <td>f9e4b658b201a9f2ecdecbb34bed034b</td>
      <td>5</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2018-02-17 00:00:00</td>
      <td>2018-02-18 14:36:24</td>
    </tr>
    <tr>
      <th>3</th>
      <td>e64fb393e7b32834bb789ff8bb30750e</td>
      <td>658677c97b385a9be170737859d3511b</td>
      <td>5</td>
      <td>NaN</td>
      <td>Recebi bem antes do prazo estipulado.</td>
      <td>2017-04-21 00:00:00</td>
      <td>2017-04-21 22:02:06</td>
    </tr>
    <tr>
      <th>4</th>
      <td>f7c4243c7fe1938f181bec41a392bdeb</td>
      <td>8e6bfb81e283fa7e4f11123a3fb894f1</td>
      <td>5</td>
      <td>NaN</td>
      <td>Parab√©ns lojas lannister adorei comprar pela I...</td>
      <td>2018-03-01 00:00:00</td>
      <td>2018-03-02 10:26:53</td>
    </tr>
  </tbody>
</table>
</div>




```python
# print(df_original.to_markdown()) #transformar panda df em md para posts
```


    ---------------------------------------------------------------------------

    ModuleNotFoundError                       Traceback (most recent call last)

    <ipython-input-13-11ffd7199387> in <module>
    ----> 1 import tabulatehelper as th
          2 
          3 print(th.md_table(df_original, formats={-1: 'c'}))
    

    ModuleNotFoundError: No module named 'tabulatehelper'



```python
df_original.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 99224 entries, 0 to 99223
    Data columns (total 7 columns):
     #   Column                   Non-Null Count  Dtype 
    ---  ------                   --------------  ----- 
     0   review_id                99224 non-null  object
     1   order_id                 99224 non-null  object
     2   review_score             99224 non-null  int64 
     3   review_comment_title     11568 non-null  object
     4   review_comment_message   40977 non-null  object
     5   review_creation_date     99224 non-null  object
     6   review_answer_timestamp  99224 non-null  object
    dtypes: int64(1), object(6)
    memory usage: 5.3+ MB
    


```python
df_original.isnull().sum()

# 99224-58247 = 40977 (58% de valores nulos)
```




    review_id                      0
    order_id                       0
    review_score                   0
    review_comment_title       87656
    review_comment_message     58247
    review_creation_date           0
    review_answer_timestamp        0
    dtype: int64



#### **Comparando a quantidade de valores nulos das duas vari√°veis em plot**


```python
import matplotlib.pyplot as plt

df_original.isna().sum().plot(kind = 'bar')
plt.title('Plot to checkout null values')
plt.show()
```


    
![png](output_9_0.png)
    


#### **i- Importando algumas bibliotecas de plotagem**

#### **ii- An√°lise Explorat√≥ria**


```python
import seaborn as sns

sns.set_palette('husl')
plt.figure(figsize=(9,6))
sns.countplot(y = df_original.review_comment_title, order= df_original.review_comment_title.value_counts().iloc[:15].index)
plt.title('Top 15 keywords')
plt.show()

# As vari√°veis review_comment_title e review_comment_message s√£o similares, por√©m a segunda apresenta uma gama maior de dados que podemos an√°lisar com a NLP, 
# e tamb√©m menos dados faltantes
```


    
![png](output_11_0.png)
    



```python
# Analisando a riqueza (em strings) das duas vari√°veis, para uma an√°lise de sentimentos isso pode ser importante

plt.hist(df_original['review_comment_message'].str.len(), label = 'review_comment_message')
plt.hist(df_original['review_comment_title'].str.len(),label = 'review_comment_title')
plt.legend()
plt.show()
```


    
![png](output_12_0.png)
    



```python
# A vari√°vel review_comment_message apresenta 58% de valores faltantes, por√©m n√£o temos como fazer tratamento desses dados por substitui√ß√£o
# A √∫nica alternativa √© dropar os dados faltantes e trabalhar com a amostra que temos

sns.set_palette('husl')
plt.figure(figsize=(9,6))
sns.countplot(y = df_original.review_comment_message, order= df_original.review_comment_message.value_counts().iloc[:15].index)
plt.title('Top 15 keywords')
plt.show()

```


    
![png](output_13_0.png)
    



```python
# Vamos primeiro fazer uma c√≥pia da df para n√£o prejudicar a original, caso precisemos dela depois
# Dropando as linhas com dados faltantes

df_copy = df_original
df_copy.dropna(inplace = True)
df_copy.head(10)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>review_id</th>
      <th>order_id</th>
      <th>review_score</th>
      <th>review_comment_title</th>
      <th>review_comment_message</th>
      <th>review_creation_date</th>
      <th>review_answer_timestamp</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>9</th>
      <td>8670d52e15e00043ae7de4c01cc2fe06</td>
      <td>b9bf720beb4ab3728760088589c62129</td>
      <td>4</td>
      <td>recomendo</td>
      <td>aparelho eficiente. no site a marca do aparelh...</td>
      <td>2018-05-22 00:00:00</td>
      <td>2018-05-23 16:45:47</td>
    </tr>
    <tr>
      <th>15</th>
      <td>3948b09f7c818e2d86c9a546758b2335</td>
      <td>e51478e7e277a83743b6f9991dbfa3fb</td>
      <td>5</td>
      <td>Super recomendo</td>
      <td>Vendedor confi√°vel, produto ok e entrega antes...</td>
      <td>2018-05-23 00:00:00</td>
      <td>2018-05-24 03:00:01</td>
    </tr>
    <tr>
      <th>19</th>
      <td>373cbeecea8286a2b66c97b1b157ec46</td>
      <td>583174fbe37d3d5f0d6661be3aad1786</td>
      <td>1</td>
      <td>N√£o chegou meu produto</td>
      <td>P√©ssimo</td>
      <td>2018-08-15 00:00:00</td>
      <td>2018-08-15 04:10:37</td>
    </tr>
    <tr>
      <th>22</th>
      <td>d21bbc789670eab777d27372ab9094cc</td>
      <td>4fc44d78867142c627497b60a7e0228a</td>
      <td>5</td>
      <td>√ìtimo</td>
      <td>Loja nota 10</td>
      <td>2018-07-10 00:00:00</td>
      <td>2018-07-11 14:10:25</td>
    </tr>
    <tr>
      <th>34</th>
      <td>c92cdd7dd544a01aa35137f901669cdf</td>
      <td>37e7875cdce5a9e5b3a692971f370151</td>
      <td>4</td>
      <td>Muito bom.</td>
      <td>Recebi exatamente o que esperava. As demais en...</td>
      <td>2018-06-07 00:00:00</td>
      <td>2018-06-09 18:44:02</td>
    </tr>
    <tr>
      <th>36</th>
      <td>08c9d79ec0eba1d252e3f52f14b8e6a9</td>
      <td>e029f708df3cc108b3264558771605c6</td>
      <td>5</td>
      <td>Bom</td>
      <td>Recomendo ,</td>
      <td>2018-06-13 00:00:00</td>
      <td>2018-06-13 22:54:44</td>
    </tr>
    <tr>
      <th>38</th>
      <td>b193ff3c9f32a01f3a0d9ae26b94d244</td>
      <td>e2e6ee1ed2d7f2f36b05d234983bd7a0</td>
      <td>5</td>
      <td>Maravilhoso!</td>
      <td>T√¥ completamente apaixonada, loja super respon...</td>
      <td>2018-08-10 00:00:00</td>
      <td>2018-08-11 00:22:13</td>
    </tr>
    <tr>
      <th>43</th>
      <td>86c5cfa7fcbde303f704b60a78ced7d6</td>
      <td>a6456e781cb962cc3f412b04de4fed7b</td>
      <td>5</td>
      <td>Entrega perfeita</td>
      <td>Muito bom. muito cheiroso.</td>
      <td>2018-06-13 00:00:00</td>
      <td>2018-06-14 17:29:03</td>
    </tr>
    <tr>
      <th>59</th>
      <td>500c05500aa275953129f49799ee5c73</td>
      <td>8a9424899aac432d80d8e580932b5ee9</td>
      <td>5</td>
      <td>MT lindo</td>
      <td>MT lindo</td>
      <td>2018-07-25 00:00:00</td>
      <td>2018-07-25 21:37:22</td>
    </tr>
    <tr>
      <th>67</th>
      <td>109b5ce2dd11bb8460eff3b86da6fefc</td>
      <td>25362fbf6aac4b01a28dee1e076acc26</td>
      <td>5</td>
      <td>√ìtimo Produto!</td>
      <td>Recomendo o vendedor...</td>
      <td>2018-08-17 00:00:00</td>
      <td>2018-08-17 21:47:08</td>
    </tr>
  </tbody>
</table>
</div>




```python
df_copy.info
```




    <bound method DataFrame.info of                               review_id                          order_id  \
    9      8670d52e15e00043ae7de4c01cc2fe06  b9bf720beb4ab3728760088589c62129   
    15     3948b09f7c818e2d86c9a546758b2335  e51478e7e277a83743b6f9991dbfa3fb   
    19     373cbeecea8286a2b66c97b1b157ec46  583174fbe37d3d5f0d6661be3aad1786   
    22     d21bbc789670eab777d27372ab9094cc  4fc44d78867142c627497b60a7e0228a   
    34     c92cdd7dd544a01aa35137f901669cdf  37e7875cdce5a9e5b3a692971f370151   
    ...                                 ...                               ...   
    99187  47e0954e156dac6512c25c6d2ecc1c66  16cbf959cfdb88c47ee2a29303547ec2   
    99192  0e7bc73fde6782891898ea71443f9904  bd78f91afbb1ecbc6124974c5e813043   
    99196  58be140ccdc12e8908ff7fd2ba5c7cb0  0ebf8e35b9807ee2d717922d5663ccdb   
    99197  51de4e06a6b701cb2be47ea0e689437b  b7467ae483dbe956fe9acdf0b1e6e3f4   
    99200  2ee221b28e5b6fceffac59487ed39348  f2d12dd37eaef72ed7b1186b2edefbcd   
    
           review_score       review_comment_title  \
    9                 4                  recomendo   
    15                5            Super recomendo   
    19                1    N√£o chegou meu produto    
    22                5                      √ìtimo   
    34                4                 Muito bom.   
    ...             ...                        ...   
    99187             5               Nota m√°xima!   
    99192             4                          üëç   
    99196             5         muito bom produto    
    99197             3  N√£o foi entregue o pedido   
    99200             2             Foto enganosa    
    
                                      review_comment_message review_creation_date  \
    9      aparelho eficiente. no site a marca do aparelh...  2018-05-22 00:00:00   
    15     Vendedor confi√°vel, produto ok e entrega antes...  2018-05-23 00:00:00   
    19                                               P√©ssimo  2018-08-15 00:00:00   
    22                                          Loja nota 10  2018-07-10 00:00:00   
    34     Recebi exatamente o que esperava. As demais en...  2018-06-07 00:00:00   
    ...                                                  ...                  ...   
    99187  Muito obrigado,\r\n\r\nExcelente atendimento,b...  2018-05-22 00:00:00   
    99192                                          Aprovado!  2018-07-04 00:00:00   
    99196  Ficamos muito satisfeitos com o produto, atend...  2018-06-30 00:00:00   
    99197  Bom dia \r\nDas 6 unidades compradas s√≥ recebi...  2018-06-05 00:00:00   
    99200  Foto muito diferente principalmente a graninha...  2018-03-28 00:00:00   
    
          review_answer_timestamp  
    9         2018-05-23 16:45:47  
    15        2018-05-24 03:00:01  
    19        2018-08-15 04:10:37  
    22        2018-07-11 14:10:25  
    34        2018-06-09 18:44:02  
    ...                       ...  
    99187     2018-05-23 00:51:43  
    99192     2018-07-05 00:25:13  
    99196     2018-07-02 23:09:35  
    99197     2018-06-06 10:52:19  
    99200     2018-05-25 01:23:26  
    
    [9839 rows x 7 columns]>




```python
# Droparei as vari√°veis que acredito n√£o apresentarem influ√™ncia na an√°lise

df_copy.drop("review_id", axis=1,inplace=True)
df_copy.drop("order_id", axis=1,inplace=True)
df_copy.drop("review_answer_timestamp", axis=1,inplace=True)
df_copy.drop("review_creation_date", axis=1,inplace=True)
df_copy.drop("review_comment_title", axis=1,inplace=True)
df_copy.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>review_score</th>
      <th>review_comment_message</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>9</th>
      <td>4</td>
      <td>aparelho eficiente. no site a marca do aparelh...</td>
    </tr>
    <tr>
      <th>15</th>
      <td>5</td>
      <td>Vendedor confi√°vel, produto ok e entrega antes...</td>
    </tr>
    <tr>
      <th>19</th>
      <td>1</td>
      <td>P√©ssimo</td>
    </tr>
    <tr>
      <th>22</th>
      <td>5</td>
      <td>Loja nota 10</td>
    </tr>
    <tr>
      <th>34</th>
      <td>4</td>
      <td>Recebi exatamente o que esperava. As demais en...</td>
    </tr>
  </tbody>
</table>
</div>




```python
df_copy.review_score.value_counts()
```




    5    5422
    1    1789
    4    1433
    3     737
    2     458
    Name: review_score, dtype: int64




```python
# O que podemos inferir em rela√ß√£o √† vari√°vel de Score?
# Temos 5 classes, mais da metade est√° na melhor classifica√ß√£o, por√©m a segunda classifica√ß√£o mais frequente est√° em p√©ssimo

import plotly.express as px
px.pie(df_copy,names='review_score',title='Distribution of Review Score',hole=0.5)
```


<div>                            <div id="2d0f5c6e-d19f-4536-bf6b-73b8a224154a" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("2d0f5c6e-d19f-4536-bf6b-73b8a224154a")) {                    Plotly.newPlot(                        "2d0f5c6e-d19f-4536-bf6b-73b8a224154a",                        [{"domain":{"x":[0.0,1.0],"y":[0.0,1.0]},"hole":0.5,"hovertemplate":"review_score=%{label}<extra></extra>","labels":[4,5,1,5,4,5,5,5,5,5,5,5,5,3,5,4,1,5,5,1,5,1,3,3,3,5,5,5,3,5,1,5,5,5,5,4,5,1,2,3,5,5,4,3,4,5,1,5,3,5,4,5,5,4,5,5,5,5,5,5,5,4,5,5,5,5,5,5,2,5,5,5,3,5,4,5,5,5,5,5,5,5,5,2,5,1,4,5,5,4,5,4,1,1,5,3,4,4,4,5,5,1,5,5,5,4,5,5,4,5,5,1,5,1,5,1,5,5,2,1,1,5,3,5,5,5,5,2,5,5,4,2,4,5,5,5,5,1,5,1,4,4,5,1,5,5,5,5,5,3,5,5,5,5,5,5,4,5,5,3,5,5,4,4,3,1,5,4,5,5,5,5,1,5,5,5,5,5,4,4,5,1,5,5,1,5,5,3,5,3,5,5,5,5,5,4,4,5,5,5,5,5,1,3,5,5,5,5,4,5,5,1,5,5,5,5,1,5,1,1,5,3,1,5,3,5,4,1,1,1,5,4,3,4,4,5,2,5,2,5,5,4,5,4,4,5,5,5,5,1,4,5,5,5,1,4,2,5,5,5,1,1,5,3,5,1,1,5,4,5,5,5,5,4,4,1,4,5,3,5,5,4,5,5,5,1,5,3,5,4,5,5,4,2,3,4,2,3,1,4,1,5,5,3,5,2,1,5,5,2,4,2,1,5,5,5,3,5,5,2,1,1,5,3,5,5,5,1,1,4,5,1,1,3,5,1,5,5,5,2,5,5,5,5,5,5,5,5,5,5,5,1,1,1,5,5,1,1,5,5,5,5,1,1,1,1,5,5,5,5,1,4,5,1,5,5,2,1,5,5,5,5,4,4,1,5,2,4,2,5,5,5,1,5,4,1,5,5,5,5,4,1,5,3,1,1,1,1,5,5,5,1,5,5,5,3,5,1,3,4,5,5,5,5,2,4,1,1,1,2,5,4,4,5,5,5,5,5,1,1,5,4,1,4,1,5,5,4,5,5,3,5,5,5,2,1,5,5,5,3,1,5,5,2,2,1,5,4,1,4,4,3,1,3,5,4,5,1,5,5,5,5,5,5,4,5,5,5,5,4,5,1,5,3,5,5,4,5,5,3,5,3,3,5,1,5,5,1,1,1,1,5,5,5,1,5,1,5,5,5,5,4,5,1,5,2,5,5,5,4,5,1,1,5,1,1,2,1,1,4,4,5,2,5,5,5,5,4,4,5,5,1,5,1,5,1,5,5,1,5,5,5,5,5,5,5,2,5,5,5,4,5,5,2,2,1,5,2,5,4,5,1,5,1,5,4,5,5,5,2,1,5,5,5,1,5,5,1,5,5,5,1,5,5,5,3,1,1,5,3,1,5,5,5,4,1,5,5,5,5,5,5,5,1,5,5,5,5,1,5,5,4,5,5,4,1,5,5,4,5,3,5,1,3,5,5,1,5,2,5,4,5,3,3,5,4,1,1,4,5,4,5,5,1,4,5,1,5,5,5,4,5,5,2,1,5,5,5,1,4,5,5,1,5,4,5,5,3,5,5,5,2,5,1,1,1,1,5,1,1,5,5,1,5,5,4,5,5,5,4,5,1,2,1,4,1,5,5,3,5,5,5,1,5,4,1,5,5,5,5,1,4,4,5,5,5,5,5,4,4,4,5,4,1,5,5,5,5,5,3,1,2,5,1,1,1,5,5,3,5,5,1,5,5,1,5,1,5,5,3,5,4,1,4,5,5,5,5,4,3,5,5,5,5,2,4,1,2,5,5,5,1,1,5,5,1,5,5,5,5,5,5,3,5,3,2,5,3,5,3,5,5,5,3,4,3,5,5,4,3,5,5,5,4,1,1,5,5,1,5,2,1,3,4,3,5,1,2,5,4,4,1,5,1,5,5,1,1,3,4,2,5,5,5,5,5,5,2,5,4,4,4,1,5,5,5,5,1,5,5,5,5,1,5,5,5,1,3,5,5,2,3,5,2,1,1,5,5,5,3,5,5,5,4,5,5,5,5,3,5,4,5,1,4,5,4,4,1,5,5,1,1,5,1,5,5,5,1,1,5,5,5,5,5,5,5,4,5,5,4,5,5,1,5,2,5,5,3,5,5,5,2,3,2,5,5,1,5,4,5,5,1,5,5,4,5,5,4,1,4,5,5,5,5,4,4,4,1,5,1,4,3,4,4,5,5,3,5,5,4,1,5,1,5,5,5,5,5,5,5,4,5,5,5,1,5,5,5,1,1,2,5,3,5,1,2,1,1,5,4,5,4,4,1,4,5,3,4,5,2,3,1,5,1,1,5,5,5,4,1,2,1,2,5,1,5,5,5,1,5,5,5,1,5,3,1,5,1,5,5,5,3,2,1,5,5,5,5,5,5,4,4,5,4,1,5,5,4,5,5,3,5,4,5,5,3,4,2,4,5,5,5,2,3,5,5,5,1,5,5,5,5,5,5,5,4,2,5,5,5,5,5,5,1,5,2,5,5,5,5,5,4,2,4,4,1,5,4,5,5,1,5,4,3,4,5,5,5,2,5,5,5,5,1,3,5,5,4,5,5,5,5,4,5,5,4,1,5,5,4,5,1,5,5,5,5,5,5,1,5,5,4,5,2,1,5,5,1,1,5,5,5,1,1,5,5,5,5,5,1,5,1,1,1,5,4,4,5,1,5,4,1,1,4,5,4,1,5,1,5,1,5,2,4,5,1,2,4,5,5,3,4,5,4,4,2,5,1,1,5,1,2,4,1,4,1,5,5,5,1,5,5,5,5,2,5,5,5,3,3,5,5,1,5,5,5,5,5,5,5,5,1,3,5,4,4,5,5,1,5,5,1,5,2,2,5,3,5,4,1,5,5,5,5,5,5,5,5,4,5,5,5,3,2,5,2,1,4,5,5,4,5,5,1,5,1,4,4,1,1,4,5,5,4,5,5,4,5,5,5,5,5,3,4,1,1,4,5,4,1,3,1,5,3,5,5,5,1,4,1,4,1,5,5,5,1,5,5,5,5,3,3,5,5,1,5,5,3,5,5,5,1,5,5,5,5,4,5,5,5,5,4,5,5,5,4,5,3,5,4,5,1,1,5,2,5,2,5,5,4,4,5,1,5,5,5,4,5,5,4,5,5,5,5,5,2,3,2,5,5,5,5,5,4,5,5,3,4,1,2,4,5,5,3,4,4,1,5,5,5,5,4,4,5,5,5,5,5,1,5,3,4,5,3,5,2,5,5,2,2,5,5,4,5,5,5,3,1,1,5,4,5,5,5,5,1,5,4,5,4,5,5,3,5,3,1,3,5,5,5,1,1,1,5,5,4,5,5,3,5,2,5,5,5,4,3,4,1,5,5,5,4,5,5,4,5,5,1,4,5,1,1,1,5,5,5,5,5,5,5,3,5,5,5,5,5,1,1,4,1,5,5,1,5,5,1,1,2,5,2,4,5,1,2,5,1,4,1,5,5,5,5,5,5,5,5,1,4,3,5,5,5,3,3,1,5,5,5,3,1,5,2,5,5,1,1,5,1,1,3,5,5,5,4,5,3,5,5,5,1,1,1,4,5,4,5,4,5,5,5,5,5,4,5,5,5,5,4,5,5,1,5,5,5,3,1,4,5,1,4,4,1,5,1,5,5,5,1,5,2,1,1,5,2,5,1,4,1,5,3,5,4,5,1,5,4,5,2,5,1,5,5,5,5,4,5,3,5,1,4,5,5,5,5,5,5,1,4,5,5,1,4,5,5,4,3,2,5,5,4,4,5,5,2,3,5,5,5,5,5,5,5,5,4,5,1,4,3,5,5,5,5,4,1,5,5,1,5,5,4,2,4,5,5,2,3,1,4,5,1,5,3,5,5,4,4,5,5,5,5,4,5,1,1,4,5,5,5,5,5,4,5,4,3,3,5,3,5,4,5,4,2,1,1,2,5,4,5,5,5,1,3,5,5,5,1,1,4,1,5,5,5,5,2,5,5,1,4,4,5,1,4,2,1,3,4,5,2,3,5,1,5,5,5,4,5,5,3,1,5,4,4,4,4,5,1,4,5,1,5,4,5,4,5,5,3,3,1,4,5,4,5,4,5,5,5,5,3,3,5,5,5,5,5,1,4,5,4,3,5,4,4,5,5,1,5,1,3,5,3,4,5,5,3,5,4,5,5,5,1,1,4,5,5,1,5,1,1,5,5,1,5,1,5,4,1,5,1,5,1,5,3,5,4,5,4,5,5,1,4,1,5,4,5,5,5,5,3,5,1,1,4,5,3,1,5,4,1,4,4,2,5,1,5,1,1,4,1,5,5,5,5,4,1,1,5,1,1,1,5,1,4,5,5,4,4,1,2,5,5,3,4,5,1,5,5,1,5,5,5,3,5,5,1,4,1,5,5,5,5,4,4,5,1,5,5,5,2,1,1,3,1,5,5,1,5,5,5,5,5,5,5,3,5,1,5,1,5,1,4,4,1,3,1,5,5,2,1,1,5,1,1,4,1,1,2,5,2,5,5,1,5,5,5,1,1,2,5,4,5,4,5,2,5,5,4,5,1,3,5,5,4,1,5,1,1,1,5,1,5,5,1,5,3,1,3,5,4,5,1,1,5,5,5,5,5,4,2,5,1,5,5,4,5,5,1,1,4,4,2,5,3,1,2,5,5,5,5,5,5,5,4,5,4,5,5,5,4,5,5,5,5,4,4,1,5,3,2,4,5,1,5,5,5,1,5,5,5,3,1,4,3,5,4,5,5,5,5,4,2,2,5,5,5,1,5,5,5,5,5,5,5,5,4,4,4,5,5,5,5,3,5,1,5,3,5,5,1,1,3,5,5,5,5,5,5,5,5,3,5,4,1,2,5,1,5,5,2,1,4,5,1,1,5,5,5,1,1,1,5,3,1,1,5,3,2,5,4,5,5,5,1,3,4,1,1,5,4,5,4,5,5,5,1,5,5,5,5,1,4,1,4,3,5,5,5,2,5,1,4,5,4,5,4,5,5,1,4,4,3,3,1,1,1,5,5,5,5,5,5,3,5,1,3,5,5,5,5,4,5,1,5,5,1,5,5,5,5,5,5,1,5,5,5,5,5,1,5,1,3,5,5,4,5,3,4,4,5,1,1,5,5,1,4,4,1,5,5,4,4,2,1,5,5,5,4,3,5,5,5,1,3,5,5,5,3,5,5,4,1,4,5,5,5,5,4,5,5,5,5,5,5,5,1,4,4,3,5,3,5,5,5,5,4,5,1,5,4,4,4,5,2,4,4,5,5,3,1,5,2,5,5,1,5,5,5,3,1,4,1,5,1,5,4,5,5,5,5,5,5,5,5,5,5,1,1,5,4,1,5,4,1,2,4,5,5,5,5,5,4,4,5,5,5,4,5,5,4,4,5,5,5,1,5,4,5,5,4,5,2,1,1,5,5,5,3,5,5,1,5,5,5,1,3,5,2,5,3,5,5,5,5,5,4,2,3,1,1,1,5,1,4,5,4,5,5,5,5,5,5,4,3,5,5,5,5,1,5,1,2,5,5,5,5,5,5,5,5,3,4,5,1,3,5,5,1,1,4,5,3,4,5,5,4,4,3,5,1,5,5,5,5,5,5,1,5,4,1,5,1,1,3,5,5,5,1,2,2,5,1,5,5,4,5,4,5,4,5,5,4,4,5,5,5,5,5,5,5,5,5,4,4,5,5,4,1,5,5,4,5,1,5,5,1,5,5,1,5,1,5,5,5,4,5,5,1,5,5,5,5,1,1,5,1,1,5,3,5,4,5,5,1,1,1,1,5,1,5,1,1,5,1,2,5,1,4,5,2,5,1,1,5,5,5,5,3,5,5,3,5,3,5,3,4,1,5,5,5,5,5,5,5,5,1,4,1,5,5,5,1,5,5,3,5,5,5,2,2,5,4,4,5,5,5,3,5,5,5,5,2,5,1,4,3,4,5,5,5,5,5,5,1,5,1,1,1,1,4,1,1,5,1,1,1,5,5,3,5,5,5,5,4,1,2,5,5,1,5,1,2,4,5,5,4,1,3,1,1,5,1,5,5,5,1,1,1,5,4,5,5,5,5,5,1,5,1,5,5,5,5,3,3,2,5,5,5,4,4,5,5,5,5,2,5,4,1,5,5,4,2,5,5,1,5,5,3,5,5,5,2,1,5,2,1,5,5,4,5,5,5,4,5,1,5,4,5,1,5,1,5,5,5,1,5,5,3,2,3,2,5,1,5,4,5,3,4,5,3,1,4,5,1,4,5,5,5,5,1,5,5,5,5,1,4,4,5,4,5,1,5,5,5,4,4,1,3,2,5,5,5,1,5,4,1,5,5,1,1,5,5,2,4,5,4,1,5,5,1,2,5,3,5,1,4,5,5,5,1,5,2,4,4,4,1,5,5,5,5,5,5,1,1,5,3,5,5,1,4,2,1,1,5,1,5,5,2,1,1,5,1,1,5,1,5,4,5,1,5,3,1,5,1,5,5,5,5,5,2,1,5,3,5,5,3,5,1,5,5,5,4,2,1,5,3,1,5,3,4,4,5,5,4,5,5,5,4,5,5,5,5,2,5,1,5,5,5,1,5,2,5,5,1,3,5,2,5,3,4,1,1,5,5,5,5,1,5,1,4,3,5,2,5,5,5,5,5,4,5,5,1,5,5,5,5,5,2,5,5,1,4,5,5,3,5,5,5,3,5,5,5,4,5,4,5,5,1,5,5,5,1,5,4,5,5,4,5,5,4,5,4,5,4,1,5,4,5,1,5,2,3,1,5,5,3,5,1,5,4,5,5,5,5,4,5,5,1,5,5,4,5,5,5,3,5,5,5,1,5,3,1,1,5,5,5,5,4,3,3,5,5,5,5,4,5,5,5,5,5,5,2,5,5,5,5,5,1,5,5,5,5,1,5,5,1,4,3,1,1,5,4,1,5,5,1,1,3,5,5,1,1,4,4,4,5,5,1,5,5,1,5,1,1,4,5,5,4,5,5,5,5,5,4,5,5,5,3,5,5,3,5,5,4,5,5,5,4,5,1,5,5,5,5,2,2,4,5,4,5,5,5,5,5,1,5,5,3,3,5,2,1,5,5,5,5,5,5,5,5,5,5,5,5,2,5,4,5,1,5,5,5,5,1,1,5,5,5,5,5,5,3,5,4,5,5,5,5,5,4,4,5,5,5,5,1,1,1,1,5,5,1,5,2,5,5,5,5,5,1,1,5,5,5,1,4,5,1,5,5,4,5,5,4,1,1,5,1,4,5,5,5,1,5,3,5,5,4,4,1,5,5,4,4,5,4,5,5,4,3,1,5,5,4,1,5,1,5,4,5,5,5,5,2,5,5,5,5,5,4,5,3,5,5,1,4,3,5,1,2,5,3,5,5,4,3,4,4,5,5,5,5,5,2,5,1,5,5,5,5,5,5,5,5,5,5,2,1,2,5,5,5,5,4,5,1,5,5,5,1,5,5,4,4,1,1,1,4,4,5,1,5,5,1,5,5,5,1,5,4,4,5,5,1,5,4,5,5,2,4,5,1,5,4,5,5,5,5,5,1,5,5,4,5,1,5,4,5,4,5,2,5,2,5,5,5,5,1,5,4,5,5,1,4,1,4,4,5,1,5,5,4,5,5,4,5,5,5,4,5,5,5,4,1,5,5,1,5,3,3,5,3,5,5,5,4,4,5,5,1,5,5,1,1,1,5,2,1,5,5,1,5,5,1,3,5,5,3,3,3,4,1,5,3,1,3,4,1,5,5,3,1,1,4,1,1,2,4,1,4,5,5,5,5,5,5,5,5,1,1,4,1,3,1,5,2,5,4,5,5,2,5,5,4,5,5,5,5,5,1,1,5,2,5,5,5,2,1,5,5,5,5,1,5,1,4,5,2,5,4,5,1,5,5,5,5,1,2,1,5,4,1,5,1,2,5,1,1,5,3,5,5,1,1,5,4,5,1,2,4,5,3,4,1,5,5,1,5,3,3,1,3,4,4,1,1,1,5,5,5,4,5,5,4,2,4,5,5,5,4,3,4,5,5,5,4,4,1,3,2,5,1,5,5,5,5,4,5,5,5,2,5,4,5,5,3,5,3,5,1,4,4,5,5,5,1,5,5,1,5,1,5,5,2,5,5,5,4,5,3,5,5,5,5,5,5,5,4,5,1,2,3,5,5,2,5,5,5,5,5,5,4,5,3,4,1,4,2,4,1,5,1,1,5,5,4,5,2,1,5,5,5,5,5,2,5,1,5,1,5,5,5,2,5,3,5,1,1,5,4,5,5,5,5,5,5,5,1,1,5,5,5,1,5,5,5,1,5,2,5,1,5,5,5,5,5,1,5,5,1,3,1,5,4,3,4,5,1,1,1,5,5,4,5,4,1,1,1,5,4,3,1,5,5,3,5,5,5,5,5,5,1,4,5,5,5,5,5,5,5,5,2,5,5,5,3,5,5,4,5,5,5,1,5,5,5,5,1,1,5,5,5,5,5,5,5,1,1,5,5,1,5,1,1,5,1,2,5,5,5,1,5,5,1,4,3,5,5,5,5,4,5,1,1,5,5,5,4,1,1,4,4,5,4,5,5,5,5,5,5,5,5,5,5,5,5,5,2,5,5,5,5,1,5,5,5,4,3,5,3,4,5,5,5,5,5,4,3,5,4,1,5,1,4,5,5,5,5,1,5,1,2,1,5,5,5,4,5,5,1,5,4,4,1,5,1,5,4,4,5,5,5,5,4,4,5,4,5,1,5,1,5,5,1,5,5,5,4,5,5,5,1,1,3,5,1,5,1,5,5,1,2,5,5,5,5,1,1,4,5,4,1,5,5,5,1,4,5,5,2,4,5,1,5,5,2,3,5,1,5,5,3,4,5,5,1,4,5,3,5,5,5,4,5,5,5,5,5,5,5,4,5,4,5,1,1,1,5,5,5,5,5,5,5,5,5,5,1,4,2,5,4,4,5,5,4,5,5,5,5,5,5,5,5,5,5,1,1,5,1,5,4,5,2,5,3,4,5,5,3,5,5,4,5,5,5,5,4,4,5,5,5,5,5,1,5,3,5,5,5,1,5,1,5,5,5,3,5,5,5,1,5,4,4,5,1,1,5,5,5,5,5,5,4,4,4,5,2,4,3,5,5,5,5,4,5,4,3,5,1,5,5,1,5,5,2,5,4,1,5,5,4,5,1,1,5,5,1,4,3,5,1,4,3,5,4,3,5,5,5,5,4,5,5,1,3,2,5,3,4,5,1,5,1,4,5,5,5,1,5,5,4,5,3,5,5,5,5,5,4,2,1,5,1,5,5,3,5,5,5,3,1,5,4,4,4,1,5,5,5,5,5,5,3,5,5,1,1,1,5,5,1,4,5,4,2,3,5,4,3,1,5,5,4,3,4,5,5,1,5,5,3,5,4,1,1,5,1,3,5,5,2,5,1,1,4,2,5,1,1,5,3,4,5,5,5,1,4,5,4,5,1,1,5,1,5,5,5,5,4,4,1,4,1,5,5,5,5,5,5,5,3,5,5,2,1,2,1,5,1,3,5,5,5,4,4,5,4,5,5,4,5,2,4,2,4,2,3,5,3,1,4,4,5,5,5,1,2,5,5,5,1,5,2,5,5,5,5,1,4,5,5,5,4,5,5,5,5,5,5,1,5,3,1,5,4,2,5,5,1,5,4,4,5,5,5,5,1,5,5,1,1,1,5,5,3,5,5,3,5,5,5,1,5,3,5,4,1,4,5,1,5,5,5,5,5,5,5,2,5,2,5,2,5,5,4,5,1,3,4,5,1,5,2,4,4,4,5,5,5,5,3,3,5,5,5,5,5,4,5,5,5,4,5,5,3,5,5,1,5,1,5,4,3,1,5,5,4,3,5,5,1,5,5,1,3,1,5,5,1,5,4,5,5,5,5,4,3,5,5,5,5,5,5,5,1,5,2,4,1,1,3,3,5,1,5,5,4,1,5,5,3,5,5,5,2,5,3,4,3,5,4,4,5,2,2,5,3,2,5,5,5,5,4,4,5,1,5,5,1,5,5,5,5,1,4,5,5,2,5,1,5,5,5,5,5,5,3,5,5,5,3,4,5,5,5,5,2,4,5,5,5,1,1,1,5,5,5,4,1,4,1,1,1,5,5,3,4,1,5,4,4,5,5,5,5,5,3,1,5,5,5,1,1,5,5,1,5,5,1,1,1,3,5,1,4,3,5,1,5,5,5,5,5,5,5,5,3,5,5,1,1,4,3,5,5,5,5,3,5,3,5,5,1,3,5,5,1,4,1,5,5,5,1,1,1,5,5,5,4,3,5,5,5,2,4,5,1,5,1,1,5,4,4,3,4,1,5,4,5,4,5,5,5,3,3,5,4,4,3,4,5,5,1,3,3,4,4,1,5,5,5,1,4,5,5,5,2,5,5,1,1,5,5,5,4,2,5,5,5,1,5,4,5,4,5,1,5,5,5,2,5,3,5,5,5,5,5,5,5,5,5,1,2,1,5,2,1,3,5,1,4,5,1,1,1,5,5,3,5,5,5,1,5,4,1,1,4,5,5,5,4,1,4,5,5,4,5,5,3,5,5,5,1,5,5,5,5,3,5,2,3,5,2,2,5,1,1,4,1,5,4,1,5,1,5,4,5,5,5,5,5,1,1,4,5,5,4,5,1,5,2,1,5,5,1,5,1,5,5,3,1,1,5,4,5,4,5,5,3,5,5,5,4,5,1,5,4,5,5,3,1,2,5,5,5,5,5,5,5,4,5,4,5,1,2,5,5,5,5,2,5,5,5,1,4,1,5,1,4,3,1,5,1,4,5,5,1,5,5,5,1,4,1,4,1,4,1,1,5,3,1,5,1,5,5,5,4,4,5,3,1,5,5,1,5,5,5,4,5,4,3,2,5,1,5,5,5,1,1,5,5,4,5,5,5,5,1,3,4,4,5,5,5,5,1,4,5,3,1,5,1,5,4,5,1,5,5,4,5,1,4,5,5,3,5,5,3,4,5,2,5,5,5,3,5,5,5,5,1,2,3,4,5,5,5,5,1,5,5,1,5,5,5,5,2,5,5,5,1,5,5,5,3,2,3,5,5,3,3,1,4,5,5,1,5,5,1,4,5,1,5,1,5,5,5,5,5,4,3,3,5,3,5,5,1,5,4,5,5,4,5,3,1,1,5,5,5,4,5,5,1,1,5,5,5,5,5,5,3,5,5,4,3,1,5,5,5,5,5,5,5,5,4,1,5,1,1,2,5,5,3,4,3,4,5,1,4,2,5,5,4,5,3,4,5,5,5,5,4,2,4,5,3,3,5,5,5,1,5,5,5,5,5,4,5,5,1,4,5,5,5,4,4,5,4,5,3,5,5,5,5,5,1,5,5,5,1,5,5,1,5,5,1,1,1,5,3,5,5,5,5,3,3,5,1,5,2,4,5,1,3,4,3,5,5,5,1,1,5,5,1,5,1,4,4,3,1,5,4,5,3,5,3,5,5,1,5,2,5,5,5,3,3,1,4,4,5,5,3,5,5,5,3,1,5,3,5,1,5,5,5,5,5,5,5,5,5,5,5,4,5,5,5,5,3,4,5,1,5,5,5,5,5,4,5,5,5,5,2,5,1,5,1,5,1,4,4,4,1,5,5,5,4,5,5,5,5,2,5,5,2,1,5,1,2,5,4,3,5,1,4,3,5,4,2,3,5,1,5,5,5,1,5,1,1,5,5,5,5,5,1,4,5,5,1,3,5,5,4,4,1,4,5,5,5,1,5,5,5,1,5,4,4,5,5,4,4,5,5,5,5,5,1,5,1,5,5,1,1,1,4,5,5,5,1,4,5,4,5,5,5,5,4,5,4,1,5,5,5,5,5,1,5,5,4,1,5,3,2,4,1,4,1,5,2,5,1,4,5,2,5,5,5,5,5,1,5,1,1,4,5,3,5,5,5,5,5,5,1,4,5,1,5,1,5,5,5,2,5,4,5,5,2,5,5,4,5,3,5,5,4,5,2,1,1,5,5,3,5,3,5,5,1,5,5,3,1,5,5,2,4,5,5,4,5,5,1,3,1,1,3,5,5,5,3,4,1,5,1,4,5,5,4,4,2,5,4,1,5,3,5,1,5,1,5,5,5,5,5,4,5,5,5,4,4,5,5,3,5,1,5,1,5,5,5,2,5,4,5,2,1,4,4,3,4,5,5,1,5,4,4,5,3,5,5,4,5,4,5,5,5,1,5,1,5,5,5,1,5,5,5,5,5,1,4,5,5,5,5,1,5,5,5,5,5,1,5,1,5,4,2,5,3,5,5,4,5,4,5,5,3,5,5,4,5,2,5,2,5,2,4,5,3,2,5,5,5,5,5,5,2,5,5,5,4,5,4,4,5,5,5,5,5,5,4,5,1,4,4,3,1,2,5,5,5,2,5,4,1,3,5,4,4,5,5,1,2,4,1,2,4,1,5,5,3,5,1,5,3,5,5,5,5,1,1,5,4,5,5,4,3,4,5,1,5,3,4,5,5,5,5,5,3,1,5,3,5,1,1,5,2,1,4,5,5,5,2,5,5,4,4,1,5,4,5,5,3,5,4,1,3,3,5,1,3,1,5,5,5,1,5,1,5,2,5,3,1,5,3,5,5,3,5,5,4,5,5,3,5,4,2,5,4,3,1,5,5,5,4,3,4,3,5,5,5,3,2,1,1,5,1,4,5,4,4,4,5,5,5,5,4,5,5,5,5,1,1,5,4,5,4,4,5,5,5,5,1,5,3,5,4,5,1,1,1,1,5,4,5,3,4,3,5,5,5,5,5,5,5,5,1,5,5,1,3,4,1,3,3,5,5,5,1,5,5,1,4,5,5,1,5,5,5,5,5,3,5,1,5,5,1,5,5,5,5,5,5,3,5,1,5,5,5,1,4,5,4,5,1,5,5,5,4,4,5,5,5,5,5,5,1,5,1,3,1,5,5,5,2,5,2,5,4,5,5,3,5,2,1,1,5,4,3,1,4,4,1,5,5,5,1,4,1,5,5,5,5,5,1,1,5,5,1,1,4,1,5,5,3,3,5,5,2,4,4,1,1,5,5,5,5,1,2,5,4,3,5,2,5,1,5,1,5,5,1,4,5,3,1,1,5,1,5,5,5,1,4,5,5,1,5,5,5,2,1,5,4,5,1,5,4,1,4,3,4,5,1,5,5,3,5,1,5,1,4,5,2,3,5,5,5,5,5,1,5,5,5,3,4,5,5,5,5,4,5,5,5,5,5,1,4,1,5,5,4,1,5,5,5,4,1,2,5,4,4,1,5,5,5,5,5,5,1,5,5,4,5,4,5,5,3,5,4,2,2,5,1,5,4,1,5,1,5,5,4,1,5,5,5,5,5,3,4,1,4,1,5,5,5,5,5,1,5,4,4,5,5,3,4,5,4,1,5,5,5,5,1,1,5,5,5,5,5,5,5,3,3,5,1,5,5,5,5,5,5,1,5,5,1,5,4,4,5,5,4,5,5,3,1,1,1,5,5,3,5,5,5,5,5,1,5,1,1,4,4,4,5,3,4,5,3,5,3,1,5,5,5,3,5,5,5,1,3,4,5,1,1,5,3,1,5,5,5,5,5,1,5,5,5,1,5,3,5,1,1,4,4,5,5,4,5,4,5,5,3,4,5,5,5,1,5,3,5,5,5,5,3,1,4,5,5,5,5,5,2,5,5,5,5,5,3,5,5,4,5,5,5,5,5,1,5,1,1,1,5,4,5,1,1,2,2,5,3,5,5,5,3,4,5,3,3,5,1,5,5,4,5,1,2,2,5,5,5,5,5,5,5,5,4,4,5,1,5,5,1,1,5,5,4,4,5,5,3,5,1,1,5,4,5,4,5,5,5,5,5,5,5,1,5,5,1,5,5,5,5,4,5,5,2,5,1,3,4,5,4,5,5,4,5,5,5,5,4,3,5,1,5,5,1,5,5,1,4,5,5,5,5,5,5,1,1,5,4,5,4,1,5,1,4,5,5,5,5,5,2,5,5,5,5,3,5,4,5,5,3,1,4,5,5,5,5,1,1,5,3,4,5,1,5,5,1,1,5,1,5,5,5,1,1,2,5,5,5,4,1,5,4,5,1,5,5,4,3,5,5,5,3,5,5,5,5,4,5,5,5,1,3,5,3,5,4,5,5,4,5,1,3,5,5,5,1,5,5,5,5,5,4,1,5,3,4,5,3,5,3,1,5,5,5,5,5,5,2,4,2,1,3,5,1,3,2,5,4,4,2,5,5,1,5,1,1,4,5,4,1,5,5,5,5,1,1,5,5,5,4,1,5,5,3,4,4,3,1,5,5,5,5,5,4,5,5,5,4,5,1,5,5,5,5,5,5,5,5,5,5,1,1,2,5,5,5,1,5,1,1,5,5,3,3,5,1,4,5,4,5,1,2,1,2,5,1,5,1,5,1,5,5,5,1,5,2,5,5,1,5,1,1,2,5,5,5,1,5,4,1,3,5,4,3,5,5,5,2,4,2,5,4,5,5,5,5,4,5,3,5,5,1,4,5,1,5,1,3,5,5,4,5,5,1,5,5,1,1,5,5,1,5,1,1,5,3,4,3,4,4,4,1,5,5,5,5,1,5,5,5,5,5,3,5,5,3,1,5,5,5,5,5,5,5,5,1,4,1,5,4,5,5,5,1,5,5,5,5,5,1,5,5,3,5,1,4,5,5,5,4,5,2,4,5,3,5,5,4,5,5,4,5,5,5,4,5,5,1,1,5,1,1,5,1,5,5,4,2,5,1,5,5,1,5,5,5,3,2,5,5,5,5,5,5,5,5,1,5,1,5,4,5,4,5,3,5,4,5,5,5,1,5,5,5,5,3,5,1,5,4,4,4,4,5,5,5,5,1,5,5,4,4,5,5,5,5,5,5,1,5,5,5,5,4,4,3,5,4,4,5,5,1,5,4,5,1,4,1,2,1,5,4,2,5,5,3,5,4,5,4,5,5,5,5,4,5,5,5,5,5,4,5,5,5,1,4,4,4,5,1,5,1,5,5,5,5,4,1,5,5,5,3,1,4,5,4,1,5,2,5,5,1,5,4,1,5,2,5,1,1,5,4,4,5,5,5,5,1,5,5,4,2,5,1,5,5,5,5,4,5,4,4,5,5,3,4,5,5,5,5,4,5,5,4,2,1,5,1,5,2,4,2,5,1,5,5,5,1,5,5,4,5,5,2,4,1,5,4,5,1,3,5,1,3,5,5,5,5,5,1,1,5,5,5,5,5,4,5,4,4,1,4,5,5,2,4,1,5,5,5,5,5,1,4,5,5,4,5,3,4,5,5,5,1,5,5,2,5,1,4,3,1,1,4,5,1,5,5,5,5,5,5,5,3,4,5,1,4,5,4,5,5,2,4,1,5,5,5,5,5,5,4,4,5,1,1,5,5,5,2,1,5,5,5,4,4,1,5,5,5,5,3,5,5,5,5,1,5,5,2,4,5,1,4,1,4,5,3,5,3,1,4,1,5,1,3,5,5,4,5,4,5,5,5,1,5,5,5,5,5,4,5,5,4,2,1,1,2,5,5,5,3,5,5,5,5,1,2,5,3,5,5,1,1,1,5,5,5,5,5,4,5,5,5,5,1,5,2,1,3,5,4,5,5,5,1,4,1,1,3,1,1,5,4,5,5,5,5,5,2,4,5,5,3,1,5,5,4,4,1,5,5,5,1,1,4,1,5,5,4,3,1,1,1,4,5,1,1,1,5,5,1,3,5,5,3,1,1,5,2,5,5,4,1,3,1,4,4,5,3,5,5,1,5,4,1,5,5,5,5,5,1,5,1,5,5,5,4,5,5,5,5,4,5,2,5,5,1,5,3,4,5,1,5,2,5,4,5,4,4,3,5,5,5,5,1,5,5,3,3,5,4,5,5,4,5,1,5,4,3,1,5,2,3,1,4,5,5,5,1,5,4,5,2,1,3,5,5,5,4,5,4,5,1,5,5,5,5,1,5,4,4,5,1,5,5,1,5,4,5,1,4,5,3,1,2,5,5,5,1,4,5,1,2,4,4,5,5,5,1,5,5,5,5,5,2,1,3,5,1,1,5,1,1,5,1,3,5,2,5,5,5,2,5,5,5,5,5,2,1,5,4,5,4,1,5,1,5,4,5,1,5,5,5,5,1,5,5,5,1,5,1,4,2,4,1,1,4,5,4,4,5,5,5,5,5,1,5,5,5,5,5,1,5,5,5,4,1,5,3,5,1,5,5,4,1,4,5,1,5,4,1,1,5,5,5,5,3,5,5,5,5,5,2,5,5,5,2,3,1,5,5,5,5,1,2,4,5,1,4,5,4,4,5,5,1,5,1,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,1,1,5,4,4,5,5,4,1,5,5,5,1,5,5,5,5,5,5,5,1,5,5,5,5,5,1,5,2,5,1,2,1,5,5,5,3,5,5,3,2,4,5,4,5,5,4,5,5,5,2,5,4,5,5,4,4,5,4,5,3,5,5,1,1,5,5,5,5,1,1,3,2,5,4,3,5,2,2,3,1,4,3,4,5,5,5,1,5,3,5,5,5,4,5,1,5,5,4,1,5,5,5,4,5,5,5,5,1,5,2,5,5,3,5,1,1,5,2,5,3,4,5,5,5,1,1,5,5,1,5,5,5,5,5,5,5,4,4,1,5,1,4,2,4,5,5,5,3,5,1,3,5,5,5,4,5,1,4,5,3,1,5,5,5,5,4,4,2,2,2,5,1,1,3,5,4,5,1,3,5,5,2,4,5,5,3,5,5,1,1,5,5,5,5,5,3,5,5,5,3,5,1,5,5,1,5,4,5,4,1,5,4,4,5,5,5,5,5,3,5,1,5,5,5,5,5,4,3,1,4,5,5,5,5,5,5,1,5,5,4,3,5,5,5,1,3,2,5,4,5,5,4,1,4,5,5,5,5,3,5,4,4,5,4,5,1,5,1,3,5,1,5,1,4,5,4,1,5,5,5,5,5,5,4,1,4,3,1,4,5,5,1,5,5,5,5,1,5,1,1,5,5,1,1,5,5,5,3,1,1,5,5,3,3,3,1,5,5,5,5,5,5,5,2,5,5,5,5,3,1,5,1,3,5,5,1,5,5,5,1,5,5,5,5,5,5,5,4,4,5,1,5,5,5,4,5,5,5,5,1,5,5,5,2,5,5,5,4,1,1,1,5,5,5,1,5,1,4,1,5,2,1,5,5,5,2,1,5,5,4,3,1,4,5,1,5,1,5,4,5,1,5,1,3,5,3,4,5,5,5,3,1,5,5,5,5,5,5,4,5,5,5,5,5,5,4,4,5,1,3,5,1,5,5,5,1,5,5,5,2,1,5,3,4,5,5,3,5,1,5,2,5,5,5,1,5,4,5,5,2,2,3,5,5,5,5,1,1,4,3,1,4,5,3,1,1,5,5,1,5,5,5,4,1,5,5,1,5,1,5,3,5,5,5,5,5,3,5,5,5,2,4,4,5,1,5,3,4,5,3,3,5,2,5,5,5,5,4,5,1,5,4,5,5,5,4,3,2,1,5,5,2,5,4,5,5,5,5,5,3,5,5,5,1,5,5,5,1,1,1,5,1,5,5,1,5,5,2,4,3,5,5,5,1,5,5,1,4,5,4,5,4,1,1,5,4,1,4,5,2,5,5,4,5,5,4,2,5,2,5,5,4,3,1,4,5,5,1,4,5,4,4,5,5,1,1,5,5,5,3,4,5,5,1,5,5,2,5,5,5,1,5,5,5,5,5,5,1,5,4,4,4,5,5,5,3,5,5,1,1,5,4,1,5,3,5,4,5,5,4,1,3,1,5,5,3,5,5,5,2,4,2,5,5,4,5,5,4,1,4,5,5,5,4,5,4,1,5,1,1,1,5,5,3,5,5,1,3,5,5,4,2,5,5,5,5,4,4,3,5,5,5,1,5,4,5,1,1,4,5,5,5,5,5,5,5,5,5,5,3,5,4,5,1,5,3,4,1,5,5,4,1,5,1,2,5,5,5,1,5,5,5,1,1,4,5,5,5,5,5,5,1,5,5,3,5,2,1,5,1,5,4,1,4,5,5,1,1,5,3,1,5,5,1,1,4,5,4,4,2,4,5,1,1,5,5,5,1,5,4,1,2,3,1,2,5,3,5,1,4,5,5,4,1,5,5,1,5,5,5,5,2,4,5,5,5,1,4,4,5,5,5,1,1,4,5,5,5,5,5,5,5,5,5,4,5,5,5,1,5,3,1,1,5,5,5,5,4,5,5,4,5,1,3,1,5,3,4,5,3,5,5,1,1,5,5,5,5,5,5,5,1,5,5,5,5,5,1,5,1,1,5,5,3,1,5,5,1,1,5,5,5,1,5,5,5,5,5,5,2,3,5,5,1,5,5,5,5,5,5,2,5,4,1,5,5,5,1,1,5,5,2,4,5,5,5,4,5,3,5,5,4,5,5,5,5,5,1,4,3,3,5,5,5,5,5,1,5,5,5,5,5,4,5,5,4,1,3,4,5,5,4,5,4,5,5,5,5,5,1,3,5,5,4,5,3,1,5,5,1,4,5,5,5,5,5,5,4,4,3,5,1,5,3,5,1,1,5,5,4,4,5,5,4,5,4,3,5,5,5,5,5,2,5,5,5,1,3,5,5,1,1,5,4,5,5,3,4,5,5,5,1,5,5,5,5,3,2,5,4,5,4,1,4,3,4,5,5,5,5,5,5,5,4,5,2,5,5,5,5,5,5,5,5,4,5,3,5,4,5,1,5,5,5,1,4,5,4,5,1,1,5,4,5,5,4,4,2,5,5,1,1,5,5,4,5,1,5,1,5,3,1,5,5,4,5,2,3,5,4,5,2,4,3,5,5,1,5,1,3,3,5,5,1,4,5,5,4,4,3,3,5,5,5,5,5,1,5,5,4,4,5,5,5,5,1,5,1,5,1,5,4,5,1,4,5,5,2,5,5,5,1,5,5,2,3,1,1,3,5,5,5,5,3,4,1,5,3,4,5,2,5,5,3,3,5,3,4,4,5,5,2,3,5,3,5,5,5,5,1,5,1,5,5,2,1,5,3,5,5,2,5,2,1,4,3,5,5,5,5,1,1,4,5,5,1,5,5,1,5,5,5,5,5,5,1,5,2,4,1,1,5,5,5,5,5,5,4,5,1,1,5,1,5,1,5,5,4,1,5,3,1,5,5,4,4,1,4,5,5,2,3,5,5,5,1,5,5,1,4,5,1,2,5,5,5,3,1,5,5,3,5,1,3,5,3,2,5,5,4,5,5,5,4,5,5,5,5,5,5,5,5,3,2,5,5,5,5,5,5,5,1,5,5,5,5,5,4,5,1,4,5,5,4,4,5,5,2,1,5,4,5,5,4,1,5,5,4,2,5,4,5,5,5,5,2,4,5,5,5,1,5,4,5,5,3,1,1,1,5,2,5,4,1,5,5,5,5,1,5,5,5,5,5,5,4,5,5,5,5,5,4,1,4,5,3,5,3,5,5,1,5,5,5,1,4,4,4,5,5,3,5,5,1,1,5,5,5,3,4,4,5,3,4,5,1,5,5,1,4,1,5,2,4,5,2,3,5,1,2,1,5,1,5,4,4,5,1,5,5,5,2,5,5,1,5,2,5,5,3,1,4,1,5,5,5,5,1,1,5,1,5,5,5,5,1,5,5,4,5,5,3,3,4,4,4,3,3,5,5,5,1,5,5,5,5,1,1,4,5,1,1,5,5,5,5,5,4,5,5,1,1,5,2,5,5,2,1,1,4,5,5,4,3,1,5,5,1,5,1,5,5,1,5,5,5,1,4,4,3,4,5,5,5,5,5,5,3,1,1,5,5,4,1,1,1,5,5,4,5,5,5,5,4,5,5,5,5,1,4,5,1,2,5,3,5,1,1,5,1,5,1,5,5,4,5,5,5,5,4,4,4,5,1,5,4,5,5,1,5,3,5,5,5,5,2,5,5,5,4,5,1,5,5,5,4,5,5,5,1,5,5,5,4,5,5,5,1,4,4,5,4,1,4,5,5,5,5,5,5,5,5,5,5,5,3,5,3,4,1,5,5,5,5,5,5,5,1,5,5,5,4,4,5,5,5,1,5,3,1,5,1,3,5,1,1,3,5,5,5,5,5,5,5,5,5,4,4,5,5,5,5,5,2,5,4,1,4,5,5,5,5,1,4,4,5,1,3,4,1,5,4,5,1,1,5,3,4,4,5,5,5,4,5,5,5,5,4,5,5,3,5,5,4,5,5,3,5,3,3,5,5,5,4,4,5,5,1,5,1,5,3,5,2,5,5,5,4,5,5,3,4,5,3,5,5,5,1,5,4,5,5,5,2,5,5,4,1,4,5,5,5,1,1,5,3,5,5,1,5,2,5,5,1,5,3,5,5,1,1,3,1,4,5,4,5,5,5,5,5,5,4,5,5,3,5,5,4,4,3,5,1,5,1,5,5,5,5,5,5,5,5,1,1,5,4,5,5,5,5,5,5,5,2,5,5,5,1,5,5,5,1,5,3,1,5,5,5,4,5,1,5,4,5,5,5,3,5,5,1,5,1,2,3,3,5,4,5,5,5,5,5,5,5,5,4,5,3,4,4,4,1,4,1,1,1,5,1,5,1,2,1,5,5,5,4,2,5,5,5,5,5,5,5,5,1,5,4,5,5,5,5,5,2,4,1,5,5,3,1,1,5,5,5,5,5,1,5,5,2,4,5,4,5,4,4,5,2,5,5,5,5,5,5,1,1,5,5,5,4,1,3,1,4,1,5,1,4,5,1,5,3,3,5,4,2,4,4,5,4,5,2,5,1,5,5,1,5,5,3,2,1,4,5,4,5,4,3,3,5,5,1,4,1,1,4,3,1,5,3,1,3,1,5,1,5,5,4,5,1,5,5,5,5,5,1,5,5,1,1,5,4,5,1,5,4,3,5,1,3,5,3,5,5,2,5,5,1,4,5,4,5,5,1,5,4,2,5,5,1,5,5,5,4,5,5,1,4,5,3,5,1,5,2,3,5,1,4,1,4,4,5,5,5,4,5,5,5,4,5,5,3,5,5,5,5,5,5,5,5,5,1,5,5,2,1,5,4,2,4,1,3,4,5,5,5,2,4,5,1,5,5,5,1,3,1,4,4,5,1,5,5,4,5,1,5,5,5,5,5,5,1,5,1,5,1,1,3,5,3,5,4,1,5,5,5,5,3,5,4,3,4,1,1,4,3,4,1,4,5,4,1,2,5,1,5,5,5,5,5,5,3,5,1,5,5,5,5,1,4,3,5,4,3,5,3,4,5,5,5,1,5,4,5,5,5,4,5,3,1,5,5,5,4,5,3,5,5,5,5,5,1,4,1,5,5,5,5,5,5,3,1,4,5,5,4,5,1,1,2,5,1,5,5,5,5,2,4,5,1,4,4,1,1,4,5,2,1,3,5,5,5,3,5,3,5,5,5,2,1,5,1,5,4,5,5,5,3,5,5,5,1,2,3,5,5,3,1,1,1,5,4,1,5,5,5,1,5,3,5,2,1,5,5,4,5,5,5,5,4,1,5,1,1,5,4,5,4,5,3,5,4,5,5,3,1,5,5,4,4,1,5,5,5,5,5,5,5,1,2,3,5,4,3,4,4,3,5,5,1,5,5,3,2,2,5,1,5,3,5,5,3,4,4,5,5,5,5,5,1,5,5,4,4,4,1,5,4,2,5,1,3,2,5,5,5,5,5,4,1,4,5,3,5,3,5,5,2,5,1,5,5,5,5,5,5,5,1,5,5,3,1,5,5,1,3,5,4,4,5,5,5,1,4,5,4,5,4,5,4,5,3,2,5,5,5,4,5,1,5,5,5,5,1,4,1,5,4,4,3,5,5,5,4,5,5,4,4,5,1,5,5,5,4,2,1,1,1,5,5,1,5,4,5,5,5,5,5,5,4,5,5,5,1,1,3,5,5,5,2,4,4,5,5,1,5,1,5,5,4,5,2,5,1,4,4,5,5,3,3,5,5,2,4,5,1,5,5,5,5,3,5,5,5,5,5,5,5,5,4,4,1,1,5,1,2,5,1,5,5,1,5,5,3,5,4,5,5,5,5,1,5,1,5,5,3,4,3,2,5,5,3,5,5,5,5,5,4,5,3,2],"legendgroup":"","name":"","showlegend":true,"type":"pie"}],                        {"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"legend":{"tracegroupgap":0},"title":{"text":"Distribution of Review Score"}},                        {"responsive": true}                    ).then(function(){

var gd = document.getElementById('2d0f5c6e-d19f-4536-bf6b-73b8a224154a');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>


#### Podemos tamb√©m analisar o conte√∫do de cada review em rela√ß√£o ao score (em ordem ascendente)
1 - P√©ssimo  
2 - Ruim  
3 - Neutro  
4 - Bom  
5 - Excelente


```python
analysis_1 = df_copy[df_copy['review_score'] == 1]['review_comment_message']
print(analysis_1.head(10))
```

    19                                               P√©ssimo
    167                                    A pe√ßa n√£o serviu
    190     Faltou 1 produto e os que recebi 1 veio quebrado
    197    aqui est√° descrevendo como entregue s√≥ que ate...
    276    Cancelaram a minha compra um dia antes da entr...
    332    Boa Noite\r\n\r\n√© lament√°vel, esta loja que t...
    426    Quando instalei o cartucho de tinta, a minha i...
    863    PRODUTO JA CHEGOU COM DEFEITO N√ÉO FUCIONOU CON...
    924                        Ainda n√£o recebi o produto...
    934    Comprei 2 produtos so mandaram 1 e n mim deram...
    Name: review_comment_message, dtype: object
    


```python
analysis_2 = df_copy[df_copy['review_score'] == 2]['review_comment_message']
print(analysis_2.head(10))
```

    336     A capa protetora n√£o √© exatamente o que eu esp...
    685     O produto n√£o est√° nas medidas corretas indica...
    818     Penso que deveria haver um modo de se comunica...
    1208    Bom dia ... comprei um bebedouro vermelho met√°...
    1337    Pedi duas carteiras. Uma veio com defeito e ou...
    1379    Boa noite na compra diz wireless 150mbs Multil...
    2391    Apesar da demora na entrega. No mais foi tudo ...
    2407    Eu pedi dois kit e s√≥ chegou apenas um o outro...
    2546                    Eu n√£o recebi minha cortina ainda
    2949    Segundo site da Pineng, consta uma rela√ß√£o de ...
    Name: review_comment_message, dtype: object
    


```python
analysis_3 = df_copy[df_copy['review_score'] == 3 ]['review_comment_message'] 
print(analysis_3.head(10))
```

    131    A entrega foi dividida em duas. N√£o houve comu...
    201                                Comecei a usar agora 
    207    Comprar um produto correto na capa mas interno...
    212    Fiz um pedido de 4 garrafas de azeite.Chegaram...
    253    Apesar de ter sido entregue rapidamente, subst...
    341    No site o produto parece ter melhor qualidade ...
    394    recebe apenas 01 pe√ßa do produto! estou precis...
    455    Fiquei feliz com. O tecido ,mas acho que falto...
    726             SOLICITEI DEVOLU√á√ÉO. PRODUTO COM DEFEITO
    969                                           Muito bom.
    Name: review_comment_message, dtype: object
    


```python
analysis_4 = df_copy[df_copy['review_score'] == 4 ]['review_comment_message'] 
print(analysis_4.head(10))
```

    9      aparelho eficiente. no site a marca do aparelh...
    34     Recebi exatamente o que esperava. As demais en...
    166     Se fosse vidro tinha quebrado; veio na caixa ...
    324                             Tudo r√°pido e eficiente.
    391       O pedido chegou antes do combinado, muito bom.
    404    trabalho com profissionalismo das lojas lannis...
    493                                            otimo\r\n
    518                 Otimo produto porem caro demais !!! 
    612                                          Boa compra.
    735    Acho que o produto deveria ter um prazo de val...
    Name: review_comment_message, dtype: object
    


```python
analysis_5 = df_copy[df_copy['review_score'] == 5 ]['review_comment_message']
print(analysis_5.head(10))
```

    15     Vendedor confi√°vel, produto ok e entrega antes...
    22                                          Loja nota 10
    36                                           Recomendo ,
    38     T√¥ completamente apaixonada, loja super respon...
    43                            Muito bom. muito cheiroso.
    59                                              MT lindo
    67                              Recomendo o vendedor... 
    79     O kit mochila patrulha canina √© lindo!! Meu ne...
    90                                         Super r√°pido.
    108                                         OK RECOMENDO
    Name: review_comment_message, dtype: object
    

#### **Reduzindo a vari√°vel Score para outra vari√°vel (classe)** 
#### *Classifica√ß√µes em Pos/Neutro/Neg*



```python
# O dataset vem com uma coluna chamada "review_score", com o score de cada uma das revis√µes (de 1 a 5). 
# Aqui adiciono a coluna "classe" para polarizar os scores em positivos, neutros e negativos.

df_copy['classe'] = [ 'Positivo' if (x>3) else 'Neutro' if (x == 3) else 'Negativo' for x in df_copy['review_score']]
df_copy.head(20)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>review_score</th>
      <th>review_comment_message</th>
      <th>classe</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>9</th>
      <td>4</td>
      <td>aparelho eficiente. no site a marca do aparelh...</td>
      <td>Positivo</td>
    </tr>
    <tr>
      <th>15</th>
      <td>5</td>
      <td>Vendedor confi√°vel, produto ok e entrega antes...</td>
      <td>Positivo</td>
    </tr>
    <tr>
      <th>19</th>
      <td>1</td>
      <td>P√©ssimo</td>
      <td>Negativo</td>
    </tr>
    <tr>
      <th>22</th>
      <td>5</td>
      <td>Loja nota 10</td>
      <td>Positivo</td>
    </tr>
    <tr>
      <th>34</th>
      <td>4</td>
      <td>Recebi exatamente o que esperava. As demais en...</td>
      <td>Positivo</td>
    </tr>
    <tr>
      <th>36</th>
      <td>5</td>
      <td>Recomendo ,</td>
      <td>Positivo</td>
    </tr>
    <tr>
      <th>38</th>
      <td>5</td>
      <td>T√¥ completamente apaixonada, loja super respon...</td>
      <td>Positivo</td>
    </tr>
    <tr>
      <th>43</th>
      <td>5</td>
      <td>Muito bom. muito cheiroso.</td>
      <td>Positivo</td>
    </tr>
    <tr>
      <th>59</th>
      <td>5</td>
      <td>MT lindo</td>
      <td>Positivo</td>
    </tr>
    <tr>
      <th>67</th>
      <td>5</td>
      <td>Recomendo o vendedor...</td>
      <td>Positivo</td>
    </tr>
    <tr>
      <th>79</th>
      <td>5</td>
      <td>O kit mochila patrulha canina √© lindo!! Meu ne...</td>
      <td>Positivo</td>
    </tr>
    <tr>
      <th>90</th>
      <td>5</td>
      <td>Super r√°pido.</td>
      <td>Positivo</td>
    </tr>
    <tr>
      <th>108</th>
      <td>5</td>
      <td>OK RECOMENDO</td>
      <td>Positivo</td>
    </tr>
    <tr>
      <th>131</th>
      <td>3</td>
      <td>A entrega foi dividida em duas. N√£o houve comu...</td>
      <td>Neutro</td>
    </tr>
    <tr>
      <th>163</th>
      <td>5</td>
      <td>Gostei da aten√ß√£o com a entrega</td>
      <td>Positivo</td>
    </tr>
    <tr>
      <th>166</th>
      <td>4</td>
      <td>Se fosse vidro tinha quebrado; veio na caixa ...</td>
      <td>Positivo</td>
    </tr>
    <tr>
      <th>167</th>
      <td>1</td>
      <td>A pe√ßa n√£o serviu</td>
      <td>Negativo</td>
    </tr>
    <tr>
      <th>170</th>
      <td>5</td>
      <td>Espero receber esta semana, o que n√£o seria t√£...</td>
      <td>Positivo</td>
    </tr>
    <tr>
      <th>185</th>
      <td>5</td>
      <td>Muito bom alta qualidade!</td>
      <td>Positivo</td>
    </tr>
    <tr>
      <th>190</th>
      <td>1</td>
      <td>Faltou 1 produto e os que recebi 1 veio quebrado</td>
      <td>Negativo</td>
    </tr>
  </tbody>
</table>
</div>



#### **Prepara√ß√£o dos dados**

* N√£o podemos trabalhar com o texto cru, pois n√£o funcionaria bem na hora da vetoriza√ß√£o e aplica√ß√£o de modelo estat√≠stico

### **Stopwords**  

* Remove palavras desnecess√°rias que n√£o carregam nenhum significado. Tais palavras, dentro de uma abordagem de NLP, s√£o irrelevantes e sua remo√ß√£o colaboram com a analise textual; 
> 
* Alguns exemplos de stopwords comuns no portugu√™s s√£o preposi√ß√µes (em, na, no, etc), artigos (a, o,os, etc), conjun√ß√µes (e, mas, etc), entre outras. 


```python
def RemoveStopWords(instancia):
    stopwords = set(nltk.corpus.stopwords.words('portuguese'))
    palavras = [i for i in instancia.split() if not i in stopwords]
    return (" ".join(palavras))
```

### **Stemming**  

* Serve para diminuirmos a palavra at√© a sua raiz/base, pois assim, conseguimos tratar as palavras originais e suas respectivas deriva√ß√µes de uma mesma maneira;
>  
* Exemplo: As palavras Correr e Corrida quando submetidas √† nossa fun√ß√£o de Stemming, ambas as palavras ser√£o diminu√≠das at√© a base Corr. 


```python
def Stemming(instancia):
    stemmer = nltk.stem.RSLPStemmer()
    palavras = []
    for w in instancia.split():
        palavras.append(stemmer.stem(w))
    return (" ".join(palavras))
```

### **Limpeza**  
* √â poss√≠vel fazer uma limpeza nas strings que possam "sujar" a frase e atrapalhar a an√°lise;
> 
* Remove strings.


```python
def Limpeza_dados(instancia):
    # remove links, pontos, virgulas,ponto e virgulas dos tweets
    instancia = re.sub(r"http\S+", "", instancia).lower().replace('.','').replace(';','').replace('-','').replace(':','').replace(')','')
    return (instancia)
```

#### **Lemmatization**  

Lematiza√ß√£o √© o processo de agrupar as diferentes formas flexionadas de uma palavra para que possam ser analisadas como um √∫nico item.

Exemplos de lematiza√ß√£o:

1. jogar, jogo e jogador todas essas 3 palavras ser√£o convertidas para jogo ap√≥s a lematiza√ß√£o;
> 

2. alterar, altern√¢ncia e alterado todas essas palavras ser√£o convertidas para altera√ß√£o ap√≥s a lematiza√ß√£o.


```python
from nltk.stem import WordNetLemmatizer
wordnet_lemmatizer = WordNetLemmatizer()

def Lemmatization(instancia):
  palavras = []
  for w in instancia.split():
    palavras.append(wordnet_lemmatizer.lemmatize(w))
  return (" ".join(palavras))
```

#### **Criando uma fun√ß√£o para aplicar as fun√ß√µes acima**


```python
def Preprocessing(instancia):
    stemmer = nltk.stem.RSLPStemmer()
    instancia = re.sub(r"http\S+", "", instancia).lower().replace('.','').replace(';','').replace('-','').replace(':','').replace(')','')
    stopwords = set(nltk.corpus.stopwords.words('portuguese'))
    palavras = [stemmer.stem(i) for i in instancia.split() if not i in stopwords]
    return (" ".join(palavras))

#Aplicando a fun√ß√£o:

df_copy['texto'] = [Preprocessing(i) for i in df_copy['review_comment_message']]
```


```python
df_copy['texto'][:50]
```




    9      aparelh efici sit marc aparelh impress 3desinf...
    15             vend confi√°vel, produt ok entreg ant praz
    19                                                  p√©ss
    22                                            loj not 10
    34     receb exat esper demal encomend outr vend atra...
    36                                               recom ,
    38     t√¥ complet apaixonada, loj sup respons confi√°vel!
    43                                             bom cheir
    59                                               mt lind
    67                                            recom vend
    79     kit mochil patrulh canin lindo!! net vai amar!...
    90                                             sup r√°pid
    108                                             ok recom
    131    entreg divid dua comunic loj cheg pens hav env...
    163                                    gost aten√ß entreg
    166    vidr quebr vei caix nenhum prote√ß dentro, caix...
    167                                             pe√ß serv
    170    esper receb semana, t√£o extravagante, por√©mm a...
    185                                   bom alt qualidade!
    190                      falt 1 produt receb 1 vei quebr
    191    cheg dentr praz produt excel qualidade! acab d...
    197                    aqu descrev entreg ate agor receb
    201                                        comec us agor
    207      compr produt corret cap intern ser outro???????
    212    fiz ped 4 garraf azeitecheg 2 dia outr 2 quas ...
    228                                                recom
    244                        sup recom produt profiss qual
    249                                   bom rap vi ja cheg
    253    apes ter sid entreg rapidamente, substitu caix...
    257               entreg dentr prazo, bom per√≠od validad
    276    cancel compr dia ant entrega, lig lannist aten...
    279      produt qual efici entreg receb ant combin recom
    281        produt bom vei dentr praz cheg 08 dia parab√©m
    298                                           entreg rap
    314    cad vez compr fic satisfeit parab√©m honest cli...
    324                                      tud r√°pid efici
    327                       crem maravilh entreg sup r√°pid
    332    boa noit lament√°vel, loj tant compr ja fiz hoj...
    336       cap prote exat esper deix desej tip mater qual
    341          sit produt parec ter melhor qual mater acab
    345    receb produt corret dentr prazo,verific caix t...
    347      parab√©m produt entreg bem ant prazoatend necess
    391                          ped cheg ant combinado, bom
    394    receb apen 01 pe√ß produto! precis 2 pe√ß restante!
    404            trabalh profission loj lannist indic cert
    414                                              tud ok!
    426    instal cartuch tinta, impres indic vazio, apre...
    430    quant sit questionar! sempr cumpr sup combin r...
    455    fiq feliz tec ,ma ach falt len√ßol cim fic comp...
    479    compr ocorr esperado, desd ped receb produt tu...
    Name: texto, dtype: object




```python
type(df_copy['texto'])
```




    pandas.core.series.Series



### **Tokenization**  

O RegexpTokenizer divide uma string em substrings usando uma express√£o regular. Por exemplo, o tokenizer a seguir forma tokens de sequ√™ncias alfab√©ticas, express√µes de dinheiro e quaisquer outras sequ√™ncias que n√£o sejam espa√ßos em branco:

from nltk.tokenize import RegexpTokenizer 

s = ‚ÄúBom muffins custam $3,88\nin Nova York. Por favor, compre para mim\nduas.\n\nObrigado.‚Äù

tokenizer = RegexpTokenizer('\w+|$[\d.]+|\S+')

tokenizer.tokenize(s) ['Bom', 'muffins', 'custo', 'em', 'Novo', 'York', '.', 'Por favor', 'comprar', 'eu', 'dois' , 'de', 'eles', '.', 'Obrigado', '.']


```python
import nltk
from nltk import RegexpTokenizer

tokenizer = RegexpTokenizer(r'\w+')

## Aplicando

df_copy['texto'] = df_copy['texto'].map(tokenizer.tokenize)
df_copy['texto'].head(10)
```




    9     [aparelh, efici, sit, marc, aparelh, impress, ...
    15     [vend, confi√°vel, produt, ok, entreg, ant, praz]
    19                                               [p√©ss]
    22                                       [loj, not, 10]
    34    [receb, exat, esper, demal, encomend, outr, ve...
    36                                              [recom]
    38    [t√¥, complet, apaixonada, loj, sup, respons, c...
    43                                         [bom, cheir]
    59                                           [mt, lind]
    67                                        [recom, vend]
    Name: texto, dtype: object




```python
df_copy.head(10)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>review_score</th>
      <th>review_comment_message</th>
      <th>classe</th>
      <th>texto</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>9</th>
      <td>4</td>
      <td>aparelho eficiente. no site a marca do aparelh...</td>
      <td>Positivo</td>
      <td>[aparelh, efici, sit, marc, aparelh, impress, ...</td>
    </tr>
    <tr>
      <th>15</th>
      <td>5</td>
      <td>Vendedor confi√°vel, produto ok e entrega antes...</td>
      <td>Positivo</td>
      <td>[vend, confi√°vel, produt, ok, entreg, ant, praz]</td>
    </tr>
    <tr>
      <th>19</th>
      <td>1</td>
      <td>P√©ssimo</td>
      <td>Negativo</td>
      <td>[p√©ss]</td>
    </tr>
    <tr>
      <th>22</th>
      <td>5</td>
      <td>Loja nota 10</td>
      <td>Positivo</td>
      <td>[loj, not, 10]</td>
    </tr>
    <tr>
      <th>34</th>
      <td>4</td>
      <td>Recebi exatamente o que esperava. As demais en...</td>
      <td>Positivo</td>
      <td>[receb, exat, esper, demal, encomend, outr, ve...</td>
    </tr>
    <tr>
      <th>36</th>
      <td>5</td>
      <td>Recomendo ,</td>
      <td>Positivo</td>
      <td>[recom]</td>
    </tr>
    <tr>
      <th>38</th>
      <td>5</td>
      <td>T√¥ completamente apaixonada, loja super respon...</td>
      <td>Positivo</td>
      <td>[t√¥, complet, apaixonada, loj, sup, respons, c...</td>
    </tr>
    <tr>
      <th>43</th>
      <td>5</td>
      <td>Muito bom. muito cheiroso.</td>
      <td>Positivo</td>
      <td>[bom, cheir]</td>
    </tr>
    <tr>
      <th>59</th>
      <td>5</td>
      <td>MT lindo</td>
      <td>Positivo</td>
      <td>[mt, lind]</td>
    </tr>
    <tr>
      <th>67</th>
      <td>5</td>
      <td>Recomendo o vendedor...</td>
      <td>Positivo</td>
      <td>[recom, vend]</td>
    </tr>
  </tbody>
</table>
</div>



#### **Transformando os tokens em senten√ßas**: necess√°rio para a vetoriza√ß√£o


```python
## Fun√ß√£o:

def combine_txt(t):
    c  = ' '.join(t)
    return c

# Aplicando:

df_copy['texto'] = df_copy['texto'].apply(lambda x: combine_txt(x))
df_copy.head(10)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>review_score</th>
      <th>review_comment_message</th>
      <th>classe</th>
      <th>texto</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>9</th>
      <td>4</td>
      <td>aparelho eficiente. no site a marca do aparelh...</td>
      <td>Positivo</td>
      <td>aparelh efici sit marc aparelh impress 3desinf...</td>
    </tr>
    <tr>
      <th>15</th>
      <td>5</td>
      <td>Vendedor confi√°vel, produto ok e entrega antes...</td>
      <td>Positivo</td>
      <td>vend confi√°vel produt ok entreg ant praz</td>
    </tr>
    <tr>
      <th>19</th>
      <td>1</td>
      <td>P√©ssimo</td>
      <td>Negativo</td>
      <td>p√©ss</td>
    </tr>
    <tr>
      <th>22</th>
      <td>5</td>
      <td>Loja nota 10</td>
      <td>Positivo</td>
      <td>loj not 10</td>
    </tr>
    <tr>
      <th>34</th>
      <td>4</td>
      <td>Recebi exatamente o que esperava. As demais en...</td>
      <td>Positivo</td>
      <td>receb exat esper demal encomend outr vend atra...</td>
    </tr>
    <tr>
      <th>36</th>
      <td>5</td>
      <td>Recomendo ,</td>
      <td>Positivo</td>
      <td>recom</td>
    </tr>
    <tr>
      <th>38</th>
      <td>5</td>
      <td>T√¥ completamente apaixonada, loja super respon...</td>
      <td>Positivo</td>
      <td>t√¥ complet apaixonada loj sup respons confi√°vel</td>
    </tr>
    <tr>
      <th>43</th>
      <td>5</td>
      <td>Muito bom. muito cheiroso.</td>
      <td>Positivo</td>
      <td>bom cheir</td>
    </tr>
    <tr>
      <th>59</th>
      <td>5</td>
      <td>MT lindo</td>
      <td>Positivo</td>
      <td>mt lind</td>
    </tr>
    <tr>
      <th>67</th>
      <td>5</td>
      <td>Recomendo o vendedor...</td>
      <td>Positivo</td>
      <td>recom vend</td>
    </tr>
  </tbody>
</table>
</div>



### **CountVectorizer (Matriz Esparsa)**  

* Ele √© usado para transformar um determinado texto em um vetor (one-hot-encoded) com base na frequ√™ncia (contagem) de cada palavra que ocorre em todo o texto. Envolve contar o n√∫mero de ocorr√™ncias que cada palavra aparece em um documento (texto).  
> 
* O resultado deste m√©todo √© uma matriz Esparsa. S√£o matrizes nas quais a maioria das posi√ß√µes s√£o preenchidas com zeros. Para essas matrizes, podemos economizar espa√ßo de mem√≥ria significativo se apenas termos diferentes de zero forem armazenados.


```python
vectorizer = CountVectorizer(analyzer="word")

matriz = vectorizer.fit_transform(df_copy['texto'])

type(matriz)
```




    scipy.sparse.csr.csr_matrix




```python
matriz.shape
```




    (9839, 6002)




```python
# Colocando os dados em outras vari√°veis para facilitar a chamada

texto = df_copy['texto']
classes = df_copy['classe']
type(texto)
```




    pandas.core.series.Series



#### **Multinomial Naive Bayes**

Naive Bayes √© o algoritmo de classifica√ß√£o mais direto e r√°pido, adequado para uma grande quantidade de dados. O classificador Naive Bayes √© usado com sucesso em v√°rias aplica√ß√µes, como filtragem de spam, classifica√ß√£o de texto, an√°lise de sentimentos e sistemas de recomenda√ß√£o. Ele usa o teorema de probabilidade de Bayes para predi√ß√£o de classe desconhecida.


```python
modelo = MultinomialNB()
modelo.fit(matriz,classes)
```




    MultinomialNB()




```python
matriz.A
```




    array([[0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 0, 0],
           ...,
           [0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 0, 0]], dtype=int64)



#### **Podemos fazer o teste do modelo em uma pequena amostra de textos**


```python
# defina inst√¢ncias de teste dentro de uma lista
testes = ['A entrega foi efetuada muito antes do prazo dado.',
          'Recebi exatamente o que esperava. As demais encomendas de outros vendedores atrasaram, mas esta chegou no prazo.',
          'N√£o recomendaria esta loja nem pretendo voltar a comprar nela.A mercadoria foi entregue ap√≥s o prazo e quase 1 m√™s ap√≥s a compra. Estranhei muito',
          'Excelente produto.',
          "Creme maravilhoso e entrega super r√°pida",
          'A embalagem deixou a desejar, por pouco o produto n√£o foi danificado, a caixa estava toda amassada.',
          'N√£o gostei do produto, material de p√©ssima qualidade.',
          'Atendimento excelente',
          'Somente foi entregue uma unidade, ao inv√©s de duas.',
          'Fiquei feliz com. O tecido ,mas acho que faltou o len√ßol de cima para ficar completo, espero que logo lancem.',
          'A bolsa e muito bonita por fora. Mas dentro n√£o tem forro e √© bem fragil',
          'Fonte veio diferente do anunciado.',
          'Produto com funcionamento correto, porem qualidade do material muito baixa.',
          'Frete muito caro.',
          'Produto Maravilhoso, exatamente como anunciado! Amei e j√° coloquei! Recomendo muito!',
          '√© muito desordenado comprei tr√™s produtos mais n√£o veio no dia sert√µes veio separado e em data separada']
```


```python
testes = [Preprocessing(i) for i in testes]
```


```python
matriz_teste = vectorizer.transform(testes)
```


```python
# Fazendo a classifica√ß√£o com o modelo treinado:

for t, c in zip (testes,modelo.predict(matriz_teste)):
    print (t +", "+ c)
```

    entreg efetu ant praz dad, Positivo
    receb exat esper demal encomend outr vend atrasaram, cheg praz, Positivo
    recomend loj pret volt compr nela mercad entreg ap√≥s praz quas 1 m√™ ap√≥s compr estranh, Negativo
    excel produt, Positivo
    crem maravilh entreg sup r√°pid, Positivo
    embal deix desejar, pouc produt danificado, caix tod amass, Neutro
    gost produto, mater p√©ss qual, Positivo
    atend excel, Positivo
    soment entreg unidade, inv√© dua, Negativo
    fiq feliz tec ,ma ach falt len√ßol cim fic completo, esper log lanc, Negativo
    bols bonit dentr forr bem fragil, Positivo
    font vei difer anunci, Negativo
    produt funcion correto, por qual mater baix, Positivo
    fret car, Positivo
    produt maravilhoso, exat anunciado! ame coloquei! recom muito!, Positivo
    desorden compr tr√™ produt vei dia sert vei separ dat separ, Negativo
    


```python
# Probabilidades de cada classe na an√°lise acima

print (modelo.classes_)
modelo.predict_proba(matriz_teste).round(2)
```

    ['Negativo' 'Neutro' 'Positivo']
    




    array([[0.  , 0.  , 0.99],
           [0.01, 0.  , 0.98],
           [0.94, 0.  , 0.06],
           [0.  , 0.  , 1.  ],
           [0.  , 0.  , 1.  ],
           [0.21, 0.59, 0.2 ],
           [0.34, 0.02, 0.64],
           [0.  , 0.  , 1.  ],
           [0.98, 0.01, 0.01],
           [0.54, 0.42, 0.04],
           [0.  , 0.01, 0.99],
           [0.63, 0.07, 0.3 ],
           [0.05, 0.02, 0.93],
           [0.16, 0.19, 0.65],
           [0.  , 0.  , 1.  ],
           [0.7 , 0.1 , 0.21]])



### **Tag de Nega√ß√µes**

* Acrescenta uma tag _NEG encontrada ap√≥s um 'n√£o'.
* Objetivo √© dar mais peso para o modelo identificar uma invers√£o de sentimento da frase.
* Exemplos: 
    - Eu n√£o gosto do partido e tamb√©m nunca votaria nesse governante!  
    ~ Eu n√£o gosto_NEG do_NEG partido_NEG e_NEG tamb√©m_NEG nunca_NEG votaria_NEG nesse_NEG governante!_NEG


```python
def marque_negacao(texto):
    negacoes = ['n√£o','not']
    negacao_detectada = False
    resultado = []
    palavras = texto.split()
    for p in palavras:
        p = p.lower()
        if negacao_detectada == True:
            p = p + '_NEG'
        if p in negacoes:
            negacao_detectada = True
        resultado.append(p)
    return (" ".join(resultado))
```

### **Criando modelos com Pipelines**

* Pipelines s√£o interessantes para reduzir c√≥digo e automatizar fluxos


```python
from sklearn.pipeline import Pipeline

pipeline_simples = Pipeline([
  ('counts', CountVectorizer()),
  ('classifier', MultinomialNB())
])
```

* Pipeline que atribui tag de negac√µes nas palavras


```python
pipeline_negacoes = Pipeline([
  ('counts', CountVectorizer(tokenizer=lambda text: marque_negacao(text))),
  ('classifier', MultinomialNB())
])
```


```python
pipeline_simples.fit(texto,classes)
```




    Pipeline(steps=[('counts', CountVectorizer()), ('classifier', MultinomialNB())])



* Etapas do pipeline


```python
pipeline_simples.steps
```




    [('counts', CountVectorizer()), ('classifier', MultinomialNB())]




```python
pipeline_negacoes.fit(texto,classes)
```




    Pipeline(steps=[('counts',
                     CountVectorizer(tokenizer=<function <lambda> at 0x0000027A53AF0310>)),
                    ('classifier', MultinomialNB())])




```python
pipeline_negacoes.steps
```




    [('counts',
      CountVectorizer(tokenizer=<function <lambda> at 0x0000027A53AF0310>)),
     ('classifier', MultinomialNB())]



* Modelo com SVM


```python
pipeline_svm_simples = Pipeline([
  ('counts', CountVectorizer()),
  ('classifier', svm.SVC(kernel='linear'))
])
```


```python
pipeline_svm_negacoes = Pipeline([
  ('counts', CountVectorizer(tokenizer=lambda text: marque_negacao(text))),
  ('classifier', svm.SVC(kernel='linear'))
])
```

#### Validando os Modelos com Valida√ß√£o Cruzada

* Fazendo o cross validation do modelo


```python
resultados = cross_val_predict(pipeline_simples, texto, classes, cv=10)
```

* Medindo a acur√°cia m√©dia do modelo


```python
metrics.accuracy_score(classes,resultados)
```




    0.8373818477487549



* Medidas de valida√ß√£o do modelo


```python
sentimento=['Positivo','Negativo','Neutro']

print(metrics.classification_report(classes,resultados,sentimento))
#dados neutros com precis√£o baixa (pouca amostra ou √© dif√≠cil pro modelo identificar uma frase "neutra"?)
```

    C:\Users\lucas\anaconda3\lib\site-packages\sklearn\utils\validation.py:70: FutureWarning:
    
    Pass labels=['Positivo', 'Negativo', 'Neutro'] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error
    
    

                  precision    recall  f1-score   support
    
        Positivo       0.89      0.94      0.91      6855
        Negativo       0.72      0.78      0.75      2247
          Neutro       0.22      0.04      0.07       737
    
        accuracy                           0.84      9839
       macro avg       0.61      0.59      0.58      9839
    weighted avg       0.80      0.84      0.81      9839
    
    

* Matriz de confus√£o: nos permite analisar onde o algoritmo est√° errando com maior frequ√™ncia


```python
print(pd.crosstab(classes, resultados, rownames=['Real'], colnames=['Predito'], margins=True))
```

    Predito   Negativo  Neutro  Positivo   All
    Real                                      
    Negativo      1751      41       455  2247
    Neutro         342      29       366   737
    Positivo       335      61      6459  6855
    All           2428     131      7280  9839
    


```python
def Metricas(modelo, texto, classes):
  resultados = cross_val_predict(modelo, texto, classes, cv=10)
  return 'Acur√°cia do modelo: {}'.format(metrics.accuracy_score(classes,resultados))
```

### **Naive Bayes**


```python
# naive bayes simples
Metricas(pipeline_simples,texto,classes)
```




    'Acur√°cia do modelo: 0.8373818477487549'




```python
# naive bayes com tag de negacoes (piorou com o uso de nega√ß√µes)
Metricas(pipeline_negacoes,texto,classes)
```




    'Acur√°cia do modelo: 0.711556052444354'



### **Support Vector Machine (SVM) ~ (funciona melhor ainda com dados bin√°rios)**

 * √â um algoritmo de aprendizado de m√°quina supervisionado que pode ser usado para desafios de classifica√ß√£o ou regress√£o. Analisa os dados e reconhece padr√µes. Linear SVM √© o mais novo algoritmo de aprendizado de m√°quina (minera√ß√£o de dados) extremamente r√°pido para resolver problemas de classifica√ß√£o multiclasse de conjuntos de dados ultra grandes que implementa uma vers√£o propriet√°ria original de um algoritmo de plano de corte para projetar uma m√°quina de vetor de suporte linear. LinearSVM √© uma rotina linearmente escal√°vel, o que significa que ela cria um modelo SVM em um tempo de CPU que escala linearmente com o tamanho do conjunto de dados de treinamento.



```python
# svm linear simples
Metricas(pipeline_svm_simples,texto,classes)
```




    'Acur√°cia do modelo: 0.8296574855168208'




```python
# svm linear com tag de negacoes (√© bem mais demorado, pois as palavras que est√£o ap√≥s um N√ÉO ir√£o duplicar)
Metricas(pipeline_svm_negacoes,texto,classes)
```




    'Acur√°cia do modelo: 0.7478402276654131'



#### **Modelo com a Tag de Nega√ß√µes**


```python
resultados = cross_val_predict(pipeline_negacoes,texto, classes, cv=10)
```


```python
metrics.accuracy_score(classes,resultados)
```




    0.711556052444354




```python
sentimento=['Positivo','Negativo','Neutro']
print (metrics.classification_report(classes,resultados,sentimento))
```

                  precision    recall  f1-score   support
    
        Positivo       0.73      0.96      0.83      6855
        Negativo       0.55      0.19      0.28      2247
          Neutro       0.09      0.01      0.02       737
    
        accuracy                           0.71      9839
       macro avg       0.46      0.39      0.38      9839
    weighted avg       0.64      0.71      0.64      9839
    
    

    C:\Users\lucas\anaconda3\lib\site-packages\sklearn\utils\validation.py:70: FutureWarning:
    
    Pass labels=['Positivo', 'Negativo', 'Neutro'] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error
    
    

* Matriz de confus√£o


```python
# Podemos comparar o resultado das duas matrizes e verificar em quais ele acertou mais em cada classe
# Nesta segunda matriz podemos ver que o modelo acertou bem menos em negativo com a tag de nega√ß√µes

print (pd.crosstab(classes, resultados, rownames=['Real'], colnames=['Predito'], margins=True))
```

    Predito   Negativo  Neutro  Positivo   All
    Real                                      
    Negativo       430      62      1755  2247
    Neutro          97       9       631   737
    Positivo       261      32      6562  6855
    All            788     103      8948  9839
    

## **Considera√ß√µes Finais**
### **√â poss√≠vel implementar nosso modelo com alguns adicionais, como:**
* Aumentar features (temos bag of words), poder√≠amos aumentar a quantidade de palavras, considerar emojis;
* Dados bin√°rios fluem muito bem com SVM, apesar do algoritmo ter funcionado um pouco melhor que NB com nossas 3 classes;
* Talvez dar uma pesquisada em m√©todos de Lemmatizer e Stemming voltados √† lingua Portuguesa;
* Sinta-se livre para modificar/aprimorar esse c√≥digo.
